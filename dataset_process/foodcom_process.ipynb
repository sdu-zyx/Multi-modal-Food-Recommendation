{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:25:50.525945Z",
     "start_time": "2025-08-29T08:25:40.559982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data preprocess start\n",
      "******************************\n"
     ]
    },
    {
     "data": {
      "text/plain": "           user_id  recipe_id        date  rating  \\\n0             2312        780  2000-09-12       5   \n1             2999       3567  2000-10-23       5   \n2             2178       3704  2000-10-30       3   \n3             2178       4366  2000-11-04       5   \n4             3794        191  2000-12-15       5   \n...            ...        ...         ...     ...   \n464732  2001513060     367414  2018-12-17       1   \n464733  2001513060     192495  2018-12-17       5   \n464734      454804      20713  2018-12-17       0   \n464735     1290903     131607  2018-12-18       5   \n464736      226867     363072  2018-12-18       5   \n\n                                                   review  \n0                                 This was delicious.\\r\\n  \n1       I have made this pie instead of plain ol' pump...  \n2       Careful not to cook it too long... you want th...  \n3       if you like oysters, this is a great alternati...  \n4       BOY!  You sure can't eat just one of these muf...  \n...                                                   ...  \n464732  maybe I did something wrong , but I thought th...  \n464733  This is a keeper. Delicious Soup that both my ...  \n464734  Made this as gifts. Did 6, quart jars plus had...  \n464735  This is a great recipe for a nice thin crispy ...  \n464736  Leaping Lizards, Lori! These are wonderful! I ...  \n\n[464737 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>recipe_id</th>\n      <th>date</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2312</td>\n      <td>780</td>\n      <td>2000-09-12</td>\n      <td>5</td>\n      <td>This was delicious.\\r\\n</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2999</td>\n      <td>3567</td>\n      <td>2000-10-23</td>\n      <td>5</td>\n      <td>I have made this pie instead of plain ol' pump...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2178</td>\n      <td>3704</td>\n      <td>2000-10-30</td>\n      <td>3</td>\n      <td>Careful not to cook it too long... you want th...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2178</td>\n      <td>4366</td>\n      <td>2000-11-04</td>\n      <td>5</td>\n      <td>if you like oysters, this is a great alternati...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3794</td>\n      <td>191</td>\n      <td>2000-12-15</td>\n      <td>5</td>\n      <td>BOY!  You sure can't eat just one of these muf...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>464732</th>\n      <td>2001513060</td>\n      <td>367414</td>\n      <td>2018-12-17</td>\n      <td>1</td>\n      <td>maybe I did something wrong , but I thought th...</td>\n    </tr>\n    <tr>\n      <th>464733</th>\n      <td>2001513060</td>\n      <td>192495</td>\n      <td>2018-12-17</td>\n      <td>5</td>\n      <td>This is a keeper. Delicious Soup that both my ...</td>\n    </tr>\n    <tr>\n      <th>464734</th>\n      <td>454804</td>\n      <td>20713</td>\n      <td>2018-12-17</td>\n      <td>0</td>\n      <td>Made this as gifts. Did 6, quart jars plus had...</td>\n    </tr>\n    <tr>\n      <th>464735</th>\n      <td>1290903</td>\n      <td>131607</td>\n      <td>2018-12-18</td>\n      <td>5</td>\n      <td>This is a great recipe for a nice thin crispy ...</td>\n    </tr>\n    <tr>\n      <th>464736</th>\n      <td>226867</td>\n      <td>363072</td>\n      <td>2018-12-18</td>\n      <td>5</td>\n      <td>Leaping Lizards, Lori! These are wonderful! I ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>464737 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(2024)\n",
    "print(\"data preprocess start\")\n",
    "print(\"*\"*30)\n",
    "\n",
    "raw_interaction = pd.read_csv('/home/zyx2509/Food_dataset/Foodcom/raw_dataset/RAW_interactions.csv')\n",
    "pp_recipe = pd.read_csv('/home/zyx2509/Food_dataset/Foodcom/raw_dataset/PP_recipes.csv')\n",
    "\n",
    "records_removed = True\n",
    "df = raw_interaction[raw_interaction['recipe_id'].isin(pp_recipe['id'].tolist())]\n",
    "\n",
    "while records_removed:\n",
    "    # 统计每个用户和物品的出现次数\n",
    "    user_counts = df['user_id'].value_counts()\n",
    "    item_counts = df['recipe_id'].value_counts()\n",
    "\n",
    "    # 筛选出现次数至少为5次的用户和物品\n",
    "    valid_users = user_counts[user_counts >= 5].index\n",
    "    valid_items = item_counts[item_counts >= 5].index\n",
    "\n",
    "    # 使用筛选后的用户和物品过滤原始数据框\n",
    "    filtered_df = df[df['user_id'].isin(valid_users) & df['recipe_id'].isin(valid_items)]\n",
    "\n",
    "    # 检查是否有记录被删除\n",
    "    if len(filtered_df) == len(df):\n",
    "        records_removed = False\n",
    "    else:\n",
    "        records_removed = True\n",
    "\n",
    "    # 更新原始数据框\n",
    "    df = filtered_df\n",
    "core_inter = df\n",
    "sort_filter_ui = core_inter.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "sort_filter_ui"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29944\n"
     ]
    },
    {
     "data": {
      "text/plain": "           user_id  recipe_id        date  rating  \\\n1             2999       3567  2000-10-23       5   \n2             2178       3704  2000-10-30       3   \n3             2178       4366  2000-11-04       5   \n6             5523       7695  2001-02-01       1   \n9            42189       4460  2001-02-12       5   \n...            ...        ...         ...     ...   \n464732  2001513060     367414  2018-12-17       1   \n464733  2001513060     192495  2018-12-17       5   \n464734      454804      20713  2018-12-17       0   \n464735     1290903     131607  2018-12-18       5   \n464736      226867     363072  2018-12-18       5   \n\n                                                   review  \n1       I have made this pie instead of plain ol' pump...  \n2       Careful not to cook it too long... you want th...  \n3       if you like oysters, this is a great alternati...  \n6                                                I agree.  \n9       I have had this before. It has really good fla...  \n...                                                   ...  \n464732  maybe I did something wrong , but I thought th...  \n464733  This is a keeper. Delicious Soup that both my ...  \n464734  Made this as gifts. Did 6, quart jars plus had...  \n464735  This is a great recipe for a nice thin crispy ...  \n464736  Leaping Lizards, Lori! These are wonderful! I ...  \n\n[428513 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>recipe_id</th>\n      <th>date</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>2999</td>\n      <td>3567</td>\n      <td>2000-10-23</td>\n      <td>5</td>\n      <td>I have made this pie instead of plain ol' pump...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2178</td>\n      <td>3704</td>\n      <td>2000-10-30</td>\n      <td>3</td>\n      <td>Careful not to cook it too long... you want th...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2178</td>\n      <td>4366</td>\n      <td>2000-11-04</td>\n      <td>5</td>\n      <td>if you like oysters, this is a great alternati...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>5523</td>\n      <td>7695</td>\n      <td>2001-02-01</td>\n      <td>1</td>\n      <td>I agree.</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>42189</td>\n      <td>4460</td>\n      <td>2001-02-12</td>\n      <td>5</td>\n      <td>I have had this before. It has really good fla...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>464732</th>\n      <td>2001513060</td>\n      <td>367414</td>\n      <td>2018-12-17</td>\n      <td>1</td>\n      <td>maybe I did something wrong , but I thought th...</td>\n    </tr>\n    <tr>\n      <th>464733</th>\n      <td>2001513060</td>\n      <td>192495</td>\n      <td>2018-12-17</td>\n      <td>5</td>\n      <td>This is a keeper. Delicious Soup that both my ...</td>\n    </tr>\n    <tr>\n      <th>464734</th>\n      <td>454804</td>\n      <td>20713</td>\n      <td>2018-12-17</td>\n      <td>0</td>\n      <td>Made this as gifts. Did 6, quart jars plus had...</td>\n    </tr>\n    <tr>\n      <th>464735</th>\n      <td>1290903</td>\n      <td>131607</td>\n      <td>2018-12-18</td>\n      <td>5</td>\n      <td>This is a great recipe for a nice thin crispy ...</td>\n    </tr>\n    <tr>\n      <th>464736</th>\n      <td>226867</td>\n      <td>363072</td>\n      <td>2018-12-18</td>\n      <td>5</td>\n      <td>Leaping Lizards, Lori! These are wonderful! I ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>428513 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "Dir = r'/home/zyx2509/Food_dataset/Foodcom/'\n",
    "image_dir = 'image_dataset'\n",
    "dst_dir = Dir + image_dir\n",
    "\n",
    "file_names = os.listdir(dst_dir)\n",
    "\n",
    "finish_image = [int(os.path.splitext(file_name)[0]) for file_name in file_names]\n",
    "sort_filter_ui = sort_filter_ui[sort_filter_ui['recipe_id'].isin(finish_image)]\n",
    "print(len(finish_image))\n",
    "sort_filter_ui"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:26:21.611986Z",
     "start_time": "2025-08-29T08:26:21.505593Z"
    }
   },
   "id": "3c6d6b13b7dbc611",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257107 42852 128554\n",
      "12764 10557 7596\n",
      "191893 35163 96143\n",
      "4821\n",
      "25832 14229 23029\n",
      "29943 4111\n",
      "1711 4000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_ = sort_filter_ui[:int(0.6*len(sort_filter_ui))]\n",
    "valid_ = sort_filter_ui[int(0.6*len(sort_filter_ui)):int(0.7*len(sort_filter_ui))]\n",
    "test_ = sort_filter_ui[int(0.7*len(sort_filter_ui)):]\n",
    "print(len(train_), len(valid_), len(test_))\n",
    "u_tr = set(train_['user_id'].tolist())\n",
    "u_te = set(test_['user_id'].tolist())\n",
    "u_total = u_tr & u_te\n",
    "print(len(u_tr), len(u_te), len(u_total))\n",
    "filter_u_tr = train_[train_['user_id'].isin(u_total)]\n",
    "filter_u_te = test_[test_['user_id'].isin(u_total)]\n",
    "filter_u_va = valid_[valid_['user_id'].isin(u_total)]\n",
    "print(len(filter_u_tr), len(filter_u_va), len(filter_u_te))\n",
    "u_va = set(filter_u_va['user_id'].tolist())\n",
    "i_tr = set(filter_u_tr['recipe_id'].tolist())\n",
    "i_te = set(filter_u_te['recipe_id'].tolist())\n",
    "i_va = set(filter_u_va['recipe_id'].tolist())\n",
    "i_total = i_tr|i_va|i_te\n",
    "i_cold = (i_va|i_te)-i_tr\n",
    "print(len(u_va))\n",
    "print(len(i_tr), len(i_va), len(i_te))\n",
    "print(len(i_total), len(i_cold))\n",
    "print(len(i_va - i_tr), len(i_te-i_tr))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:26:24.082286Z",
     "start_time": "2025-08-29T08:26:23.946137Z"
    }
   },
   "id": "28795a64bb774721",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "user_ids = list(u_total)\n",
    "user_encoder = LabelEncoder().fit(user_ids)\n",
    "\n",
    "user_to_idx = {v: i for i, v in enumerate(user_encoder.classes_)}\n",
    "idx_to_user = {i: v for i, v in enumerate(user_encoder.classes_)}\n",
    "\n",
    "item_ids = list(i_total)\n",
    "item_encoder = LabelEncoder().fit(item_ids)\n",
    "\n",
    "item_to_idx = {v: i for i, v in enumerate(item_encoder.classes_)}\n",
    "idx_to_item = {i: v for i, v in enumerate(item_encoder.classes_)}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:26:26.192636Z",
     "start_time": "2025-08-29T08:26:25.689842Z"
    }
   },
   "id": "ff89ac4bbd1c4490",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191893\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_interaction = filter_u_tr\n",
    "\n",
    "test_interaction = filter_u_te\n",
    "\n",
    "valid_interaction = filter_u_va\n",
    "\n",
    "print(len(train_interaction))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:26:27.223825Z",
     "start_time": "2025-08-29T08:26:27.222560Z"
    }
   },
   "id": "fec974353d628f78",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1740231/583291010.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_interaction['u'] = train_interaction['user_id'].map(user_to_idx)\n",
      "/tmp/ipykernel_1740231/583291010.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_interaction['i'] = train_interaction['recipe_id'].map(item_to_idx)\n",
      "/tmp/ipykernel_1740231/583291010.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_to_save['rating'] = train_to_save['rating'].astype(int)\n",
      "/tmp/ipykernel_1740231/583291010.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_interaction['u'] = test_interaction['user_id'].map(user_to_idx)\n",
      "/tmp/ipykernel_1740231/583291010.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_interaction['i'] = test_interaction['recipe_id'].map(item_to_idx)\n",
      "/tmp/ipykernel_1740231/583291010.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_to_save['rating'] = test_to_save['rating'].astype(int)\n",
      "/tmp/ipykernel_1740231/583291010.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_interaction['u'] = valid_interaction['user_id'].map(user_to_idx)\n",
      "/tmp/ipykernel_1740231/583291010.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_interaction['i'] = valid_interaction['recipe_id'].map(item_to_idx)\n",
      "/tmp/ipykernel_1740231/583291010.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  valid_to_save['rating'] = valid_to_save['rating'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/zyx2509/Food_dataset/Foodcom/'\n",
    "\n",
    "train_interaction['u'] = train_interaction['user_id'].map(user_to_idx)\n",
    "train_interaction['i'] = train_interaction['recipe_id'].map(item_to_idx)\n",
    "train_sorted = train_interaction.sort_values(by=['u', 'i'])\n",
    "train_to_save = train_sorted[['u', 'i', 'rating']]\n",
    "train_to_save['rating'] = train_to_save['rating'].astype(int)\n",
    "train_to_save.to_csv(file_path+'processed_dataset/data.train.rating', sep='\t', header=False, index=False)\n",
    "\n",
    "test_interaction['u'] = test_interaction['user_id'].map(user_to_idx)\n",
    "test_interaction['i'] = test_interaction['recipe_id'].map(item_to_idx)\n",
    "test_sorted = test_interaction.sort_values(by=['u', 'i'])\n",
    "test_to_save = test_sorted[['u', 'i', 'rating']]\n",
    "test_to_save['rating'] = test_to_save['rating'].astype(int)\n",
    "test_to_save.to_csv(file_path+'processed_dataset/data.test.rating', sep='\t', header=False, index=False)\n",
    "\n",
    "valid_interaction['u'] = valid_interaction['user_id'].map(user_to_idx)\n",
    "valid_interaction['i'] = valid_interaction['recipe_id'].map(item_to_idx)\n",
    "valid_sorted = valid_interaction.sort_values(by=['u', 'i'])\n",
    "valid_to_save = valid_sorted[['u', 'i', 'rating']]\n",
    "valid_to_save['rating'] = valid_to_save['rating'].astype(int)\n",
    "valid_to_save.to_csv(file_path+'processed_dataset/data.valid.rating', sep='\t', header=False, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:26:29.541713Z",
     "start_time": "2025-08-29T08:26:28.645788Z"
    }
   },
   "id": "37bac99dc84c08fe",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 23029 25832\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "train_df = pd.read_csv('/home/zyx2509/Food_dataset/Foodcom/processed_dataset/data.train.rating', names=['u', 'i', 'r'], sep='\\t')\n",
    "test_df = pd.read_csv('/home/zyx2509/Food_dataset/Foodcom/processed_dataset/data.test.rating', names=['u', 'i', 'r'], sep='\\t')\n",
    "\n",
    "test_i = test_df['i'].unique()\n",
    "train_i = train_df['i'].unique()\n",
    "\n",
    "cold = set(test_i) - set(train_i)\n",
    "print(len(cold), len(test_i), len(train_i))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:26:32.070690Z",
     "start_time": "2025-08-29T08:26:31.992743Z"
    }
   },
   "id": "cfc50056243a1d0a",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29943\n",
      "            id                                     ingredient_ids      i  \\\n",
      "52588       40               [6906, 4303, 2919, 2832, 3829, 1567]      0   \n",
      "26668       49  [2847, 840, 1254, 2683, 4593, 1257, 6270, 4836...      1   \n",
      "101951      58  [451, 7557, 2379, 6270, 5319, 437, 836, 3184, ...      2   \n",
      "110880      66  [739, 4053, 6009, 2045, 5010, 3203, 2066, 3783...      3   \n",
      "84384      142  [6906, 4574, 2499, 106, 7470, 63, 7407, 335, 6...      4   \n",
      "...        ...                                                ...    ...   \n",
      "141784  522871                [756, 7655, 7654, 6696, 6248, 6906]  29938   \n",
      "126326  522889          [3559, 2499, 4333, 4883, 6270, 1118, 840]  29939   \n",
      "88162   532736                           [7474, 6889, 7702, 5695]  29940   \n",
      "170005  532740                     [7998, 5476, 7642, 3723, 3497]  29941   \n",
      "77830   536119  [1647, 5915, 6335, 4493, 6413, 5434, 6696, 608...  29942   \n",
      "\n",
      "                                          ingredient_list  \n",
      "52588                [6906, 4303, 2919, 2832, 3829, 1567]  \n",
      "26668   [2847, 840, 1254, 2683, 4593, 1257, 6270, 4836...  \n",
      "101951  [451, 7557, 2379, 6270, 5319, 437, 836, 3184, ...  \n",
      "110880  [739, 4053, 6009, 2045, 5010, 3203, 2066, 3783...  \n",
      "84384   [6906, 4574, 2499, 106, 7470, 63, 7407, 335, 6...  \n",
      "...                                                   ...  \n",
      "141784                [756, 7655, 7654, 6696, 6248, 6906]  \n",
      "126326          [3559, 2499, 4333, 4883, 6270, 1118, 840]  \n",
      "88162                            [7474, 6889, 7702, 5695]  \n",
      "170005                     [7998, 5476, 7642, 3723, 3497]  \n",
      "77830   [1647, 5915, 6335, 4493, 6413, 5434, 6696, 608...  \n",
      "\n",
      "[29943 rows x 4 columns]\n",
      "7     3663\n",
      "8     3594\n",
      "9     3530\n",
      "6     3265\n",
      "10    3043\n",
      "5     2702\n",
      "11    2511\n",
      "12    1895\n",
      "4     1835\n",
      "13    1387\n",
      "14     926\n",
      "15     600\n",
      "16     408\n",
      "17     236\n",
      "18     153\n",
      "19     100\n",
      "20      69\n",
      "3       26\n",
      "Name: ingre_num, dtype: int64\n",
      "4963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29943it [00:01, 25230.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4963\n",
      "3 20 8.818121096750492\n"
     ]
    }
   ],
   "source": [
    "# algo 1\n",
    "from collections import Counter, defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "ingre_num_df = pp_recipe[['id','ingredient_ids']]\n",
    "ingre_num_df = ingre_num_df[ingre_num_df['id'].isin(item_to_idx.keys())]\n",
    "print(len(ingre_num_df))\n",
    "\n",
    "ingre_num_df['i'] = ingre_num_df['id'].map(item_to_idx)\n",
    "ingre_num_df['ingredient_list'] = ingre_num_df['ingredient_ids'].apply(lambda x: eval(x))\n",
    "\n",
    "ingre_num_df['i'] = ingre_num_df['id'].map(item_to_idx)\n",
    "ingre_num_df = ingre_num_df.sort_values(by=['i'])\n",
    "print(ingre_num_df)\n",
    "train_ingre_df = ingre_num_df[ingre_num_df['i'].isin(train_i)]\n",
    "\n",
    "df_ing_list = train_ingre_df['ingredient_list'].tolist()\n",
    "all_ingre_list = [item for sublist in df_ing_list for item in sublist]\n",
    "frequency_count = Counter(all_ingre_list)\n",
    "filtered_ingre = set([key for key, value in frequency_count.items() if value >= 1])\n",
    "\n",
    "ingre_num_df['ingredient_del'] = ingre_num_df['ingredient_list'].apply(\n",
    "    lambda x: [i for i in x if i in filtered_ingre])\n",
    "ingre_num_df['ingre_num'] = ingre_num_df['ingredient_del'].apply(lambda x: len(x))\n",
    "\n",
    "print(ingre_num_df['ingre_num'].value_counts())\n",
    "print(len(filtered_ingre))\n",
    "\n",
    "new_ingre = []\n",
    "for index, row in tqdm(ingre_num_df.iterrows()):\n",
    "    if row['ingre_num']==0:\n",
    "        new_ingre.extend(row['ingredient_list'])\n",
    "    else:\n",
    "        new_ingre.extend(row['ingredient_del']) \n",
    "\n",
    "final_ingre = set(new_ingre)\n",
    "ingre_num_df['new_ingres'] = ingre_num_df['ingredient_list'].apply(\n",
    "    lambda x: [i for i in x if i in final_ingre])\n",
    "ingre_num_df['new_ingres_num'] = ingre_num_df['new_ingres'].apply(lambda x: len(x))\n",
    "\n",
    "print(len(final_ingre))\n",
    "print(ingre_num_df['new_ingres_num'].min(), ingre_num_df['new_ingres_num'].max(), ingre_num_df['new_ingres_num'].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:26:36.867714Z",
     "start_time": "2025-08-29T08:26:34.605077Z"
    }
   },
   "id": "fa63eedf92dea770",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29943, 20) 4963\n",
      "[[4277 2698 1845 ... 4963 4963 4963]\n",
      " [1789  510  776 ... 4963 4963 4963]\n",
      " [ 274 4681 1495 ... 4963 4963 4963]\n",
      " ...\n",
      " [4641 4265 4762 ... 4963 4963 4963]\n",
      " [4951 3427 4732 ... 4963 4963 4963]\n",
      " [1024 3678 3941 ... 4963 4963 4963]] 4963 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29943it [01:51, 268.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264041\n",
      "(264041, 2)\n",
      "NumPy array saved as /home/zyx2509/Food_dataset/Foodcom/processed_dataset/ri_graph.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "save_ingre_num = ingre_num_df[['i', 'new_ingres_num']]\n",
    "save_ingre_num.to_csv(file_path + 'processed_dataset/data_id_ingre_num_file', sep='\t', header=False, index=False)\n",
    "\n",
    "ingre_ids = list(final_ingre)\n",
    "ingre_encoder = LabelEncoder().fit(ingre_ids)\n",
    "\n",
    "ingre_to_idx = {v: i for i, v in enumerate(ingre_encoder.classes_)}\n",
    "idx_to_ingre = {i: v for i, v in enumerate(ingre_encoder.classes_)}\n",
    "ingre_num_df['ingredient_idx'] = ingre_num_df['new_ingres'].apply(lambda x: [ingre_to_idx[i] for i in x])\n",
    "\n",
    "dict_data = dict(zip(ingre_num_df['i'], ingre_num_df['ingredient_idx'].tolist()))\n",
    "\n",
    "fill_value = len(ingre_ids)\n",
    "ingre_code = np.full((len(dict_data), ingre_num_df['new_ingres_num'].max()), fill_value)\n",
    "\n",
    "for i, (key, value) in enumerate(dict_data.items()):\n",
    "    ingre_code[key, :len(value)] = value\n",
    "\n",
    "# save id ingre_code file\n",
    "# total code num = 33147 \n",
    "np.save(file_path + 'processed_dataset/data_ingre_code_file.npy', ingre_code)\n",
    "\n",
    "print(ingre_code.shape, np.max(ingre_code))\n",
    "print(ingre_code, np.max(ingre_code), np.min(ingre_code))\n",
    "ri_list = []\n",
    "for i, sublist in tqdm(enumerate(ingre_code)):\n",
    "    for j in sublist:\n",
    "        if j != np.max(ingre_code):\n",
    "            ri_list.append([i, j])\n",
    "print(len(ri_list))\n",
    "ri_array = np.array(ri_list).astype(np.int_)\n",
    "print(ri_array.shape)\n",
    "\n",
    "ri_path = file_path + 'processed_dataset/ri_graph.txt'\n",
    "with open(ri_path, 'w') as f:\n",
    "    np.savetxt(f, ri_array, fmt='%d')\n",
    "print(f\"NumPy array saved as {ri_path}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T08:28:32.864988Z",
     "start_time": "2025-08-29T08:26:40.839450Z"
    }
   },
   "id": "de3d1b8c1741b716",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import torch\n",
    "\n",
    "# 打开.pkl文件并读取数据\n",
    "with open(file_path+'raw_dataset/ingr_map.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# 使用读取的数据\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "# 加载T5模型和tokenizer\n",
    "model_name = '/home/zyx2509/CIKM_code/t5-small'  # 可以选择其他T5变体和大小\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5EncoderModel.from_pretrained(model_name).cuda()\n",
    "\n",
    "def t5_encoder(row):\n",
    "    inputs = tokenizer(row, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "\n",
    "    # 生成表示\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    model_output = outputs.last_hidden_state\n",
    "    t5_out = torch.mean(model_output, dim=1, keepdim=False).squeeze().detach().cpu().numpy()\n",
    "\n",
    "    return t5_out\n",
    "    # 存储特征向量\n",
    "\n",
    "data['t5_out'] = data['processed'].apply(lambda x: t5_encoder(x))\n",
    "\n",
    "print(data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a80695ff92c753cc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def calculate_mean(arrays):\n",
    "    # 使用 np.mean 计算 NumPy 数组的平均值\n",
    "    return np.mean(arrays, axis=0)\n",
    "result_df = data.groupby('id')['t5_out'].apply(calculate_mean).reset_index()\n",
    "result_df.columns = ['id', 'MeanArray']\n",
    "result_df = result_df[result_df['id'].isin(ingre_ids)]\n",
    "result_df['idx'] = result_df['id'].apply(lambda x:ingre_to_idx[x])\n",
    "result_df = result_df.sort_values(['idx'])\n",
    "print(result_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "550275fd8f149f0d",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "ingre_feature = np.array(result_df['MeanArray'].tolist())\n",
    "# 保存数组到.npy文件中\n",
    "np.save(file_path+'processed_dataset/data_ingre_features_t5.npy', ingre_feature)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aa62771f4e5ef61",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "raw_recipe = pd.read_csv('/home/zyx2509/Food_dataset/Foodcom/raw_dataset/RAW_recipes.csv')\n",
    "\n",
    "texture_df = raw_recipe[raw_recipe['id'].isin(i_total)]\n",
    "\n",
    "texture_df = texture_df[['id', 'name', 'steps', 'description']]\n",
    "texture_df['i'] = texture_df['id'].map(item_to_idx)\n",
    "texture_df = texture_df.sort_values(by=['i'])\n",
    "\n",
    "title_dict = defaultdict(list)\n",
    "direction_dict = defaultdict(list)\n",
    "description_dict = defaultdict(list)\n",
    "\n",
    "for idx, row in tqdm(texture_df.iterrows()):\n",
    "    recipe_id = row['i']\n",
    "    title_dict[recipe_id] += [row['name']]\n",
    "    direction = eval(row['steps'])\n",
    "    direction_dict[recipe_id] += direction\n",
    "    description_dict[recipe_id] += [row['description']] \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3179362b0e57733a",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# 存储文本向量的列表\n",
    "title_vectors = {}\n",
    "\n",
    "# 遍历图片文件夹\n",
    "for key, value in tqdm(title_dict.items()):\n",
    "    inputs = tokenizer(value, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    inputs = {key: value.cuda() for key, value in inputs.items()}\n",
    "\n",
    "    # 生成表示\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    model_output = outputs.last_hidden_state\n",
    "\n",
    "    # 存储特征向量\n",
    "    title_vectors[key] = torch.mean(model_output, dim=1, keepdim=False).squeeze().detach().cpu().numpy()\n",
    "\n",
    "print(len(title_vectors))\n",
    "\n",
    "title_feature = np.zeros((len(title_vectors), title_vectors[0].shape[0]))\n",
    "\n",
    "for i, (key, value) in enumerate(title_vectors.items()):\n",
    "    title_feature[key] = value\n",
    "\n",
    "print(title_feature.shape)\n",
    "np.save(file_path + 'processed_dataset/data_title_features_t5.npy', title_feature)\n",
    "print(f\"save title feature:{title_feature.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f30613dc74b229",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "text_feature = np.zeros((len(ingre_code), ingre_feature.shape[1]))\n",
    "for i, ingre_list in enumerate(ingre_code):\n",
    "    mask = ingre_list != len(ingre_to_idx)\n",
    "    new_ingre = ingre_list[mask]\n",
    "    ingre_embs = ingre_feature[new_ingre]\n",
    "    ingre_embs = np.concatenate((ingre_embs, np.expand_dims(title_feature[i], axis=0)), axis=0)\n",
    "    text_feature[i] = ingre_embs.mean(0)\n",
    "print(text_feature.shape)\n",
    "np.save(file_path + 'processed_dataset/data_text_features_t5.npy', text_feature)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "18a883c38b9c3c42",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "filename = file_path + 'processed_dataset/mapping_dict.pkl'\n",
    "\n",
    "# 将字典保存到 pkl 文件中\n",
    "with open(filename, 'wb') as f:  # 注意要以二进制写入模式打开文件\n",
    "    pickle.dump((user_to_idx, item_to_idx, ingre_to_idx), f)\n",
    "print(\"save mapping dict\")\n",
    "print(len(ingre_to_idx), len(user_to_idx), len(item_to_idx))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2427b8a821c77b4",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29943it [10:43, 46.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# 加载预训练的ResNet-50模型\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "# 将模型设置为评估模式\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet.cuda()\n",
    "resnet.eval()\n",
    "\n",
    "# 图片文件夹路径\n",
    "image_folder = file_path+'image_dataset/'\n",
    "\n",
    "# 数据预处理\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# 存储图片向量的列表\n",
    "image_vectors = {}\n",
    "\n",
    "# 遍历图片文件夹\n",
    "num=0\n",
    "for index, row in tqdm(ingre_num_df.iterrows()):\n",
    "    image_id = row['id']\n",
    "    image_path = os.path.join(image_folder, f\"{image_id}.jpg\")\n",
    "    # 加载并预处理图片\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "\n",
    "    try:\n",
    "        image = preprocess(image)\n",
    "        # 增加一个维度，以符合模型的输入要求\n",
    "        image = image.unsqueeze(0).cuda()\n",
    "        # 使用模型提取特征\n",
    "        with torch.no_grad():\n",
    "            output = resnet(image)\n",
    "        # 提取特征向量，并转换为numpy数组\n",
    "    except Exception as e:\n",
    "        print(e, image_path)\n",
    "        image = image.convert('RGB')\n",
    "        image = preprocess(image)\n",
    "        # 增加一个维度，以符合模型的输入要求\n",
    "        image = image.unsqueeze(0).cuda()\n",
    "        # 使用模型提取特征\n",
    "        with torch.no_grad():\n",
    "            output = resnet(image)\n",
    "\n",
    "    vector = output.squeeze().detach().cpu().numpy()\n",
    "    # 存储特征向量\n",
    "    image_vectors[image_id] = vector\n",
    "\n",
    "print(len(image_vectors))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:09:31.304131Z",
     "start_time": "2025-08-29T09:58:47.197102Z"
    }
   },
   "id": "2726af90d8ca6c2a",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n",
      "(29943, 2048)\n"
     ]
    }
   ],
   "source": [
    "convert_image_vectors = {item_to_idx[key]: value for key, value in image_vectors.items()}\n",
    "\n",
    "sorted_convert_image_vectors = dict(sorted(convert_image_vectors.items()))\n",
    "print(sorted_convert_image_vectors[0].shape)\n",
    "\n",
    "image_feature = np.zeros((len(sorted_convert_image_vectors), sorted_convert_image_vectors[0].size))\n",
    "\n",
    "# 遍历字典，将值填充到数组中\n",
    "for i, (key, value) in enumerate(sorted_convert_image_vectors.items()):\n",
    "        image_feature[key] = value\n",
    "\n",
    "print(image_feature.shape)\n",
    "np.save(file_path+'processed_dataset/data_image_features_float.npy', image_feature)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:09:57.428266Z",
     "start_time": "2025-08-29T10:09:56.911641Z"
    }
   },
   "id": "49831c8f34ce8904",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "191893it [00:07, 25196.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7596/7596 [00:29<00:00, 258.81it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7596/7596 [00:22<00:00, 342.47it/s]\n"
     ]
    }
   ],
   "source": [
    "file_path = '/home/zyx2509/Food_dataset/Foodcom/processed_dataset/'\n",
    "data_file = file_path+'data.train.rating'\n",
    "test_file = file_path+'data.test.rating'\n",
    "valid_file = file_path+'data.valid.rating'\n",
    "\n",
    "train_df = pd.read_csv(data_file, names=['user', 'item', 'rate'], sep='\\t')\n",
    "test_df = pd.read_csv(test_file, names=['user', 'item', 'rate'], sep='\\t')\n",
    "valid_df = pd.read_csv(valid_file, names=['user', 'item', 'rate'], sep='\\t')\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "item_count = defaultdict(int)\n",
    "user_items = {}\n",
    "\n",
    "# 遍历 DataFrame 中的每一行\n",
    "for index, row in tqdm(train_df.iterrows()):\n",
    "        # 提取第一列的值作为聚类的键\n",
    "        u = int(row['user'])\n",
    "        # 如果键不存在于字典中，则将其初始化为一个空列表\n",
    "        if u not in user_items:\n",
    "            user_items[u] = []\n",
    "        # 将第二列的值添加到对应键的列表中\n",
    "        user_items[u].append(int(row['item']))\n",
    "        item_count[int(row['item'])] += 1\n",
    "\n",
    "print(len(item_count)) \n",
    "\n",
    "all_item = list(item_count.keys())\n",
    "count = list(item_count.values())\n",
    "sum_value = np.sum([x for x in count])\n",
    "popularity = [value / sum_value for value in count]\n",
    "\n",
    "sum_prob = np.sum([x**0.7 for x in popularity])\n",
    "probability = [value**0.7/sum_prob for value in popularity]\n",
    "\n",
    "np.random.seed(2024)\n",
    "item_set = list(item_count.keys())\n",
    "test_num = 500\n",
    "sample_type = 'popular bias'\n",
    "test_neg_items = defaultdict()\n",
    "valid_neg_items = defaultdict()\n",
    "valid_user_set = set(valid_df['user'])\n",
    "test_user_set = set(test_df['user'])\n",
    "\n",
    "for user, user_seq in tqdm(user_items.items()):\n",
    "    if user not in test_user_set:\n",
    "        continue\n",
    "    test_samples = []\n",
    "    while len(test_samples) < test_num:\n",
    "        sample_ids = np.random.choice(all_item, 2*test_num, replace=False, p=probability)\n",
    "        sample_ids_set = set(sample_ids)\n",
    "        user_seq_set = set(user_seq)\n",
    "        result_set = sample_ids_set - user_seq_set\n",
    "        test_samples = list(result_set)\n",
    "    test_samples = test_samples[:test_num]\n",
    "    test_neg_items[user] = test_samples\n",
    "\n",
    "for user, user_seq in tqdm(user_items.items()):\n",
    "    if user not in valid_user_set:\n",
    "        continue\n",
    "    valid_samples = []\n",
    "    while len(valid_samples) < test_num:\n",
    "        sample_ids = np.random.choice(all_item, 2*test_num, replace=False, p=probability)\n",
    "        sample_ids_set = set(sample_ids)\n",
    "        user_seq_set = set(user_seq)\n",
    "        result_set = sample_ids_set - user_seq_set\n",
    "        valid_samples = list(result_set)\n",
    "    valid_samples = valid_samples[:test_num]\n",
    "    valid_neg_items[user] = valid_samples"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:12:55.724477Z",
     "start_time": "2025-08-29T10:11:55.899087Z"
    }
   },
   "id": "ab9bbbd6e493c9d8",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35163it [00:01, 25082.22it/s]\n",
      "96143it [00:03, 26737.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            KeyValue\n",
      "0  0:[590, 632, 1326, 1691, 2121, 2300, 2745, 284...\n",
      "1                                          1:[24125]\n",
      "2                                     2:[2374, 5285]\n",
      "3                                           3:[3172]\n",
      "4                            4:[10170, 18628, 23178]\n",
      "                                            KeyValue\n",
      "0  0:[76, 3740, 6171, 6584, 6867, 6886, 8996, 152...\n",
      "2                                      2:[57, 10237]\n",
      "4                                           4:[3090]\n",
      "7                             7:[2432, 16061, 24274]\n",
      "9  9:[10735, 11113, 12140, 12311, 13840, 19339, 1...\n"
     ]
    }
   ],
   "source": [
    "test_pos_dict = {}\n",
    "valid_pos_dict = {}\n",
    "\n",
    "for index, row in tqdm(valid_df.iterrows()):\n",
    "        # 提取第一列的值作为聚类的键\n",
    "        u = int(row['user'])\n",
    "        if u not in valid_pos_dict:\n",
    "            valid_pos_dict[u] = []\n",
    "        valid_pos_dict[u].append(int(row['item']))\n",
    "\n",
    "for index, row in tqdm(test_df.iterrows()):\n",
    "        # 提取第一列的值作为聚类的键\n",
    "        u = int(row['user'])\n",
    "        if u not in test_pos_dict:\n",
    "            test_pos_dict[u] = []\n",
    "        test_pos_dict[u].append(int(row['item']))\n",
    "        \n",
    "test_sample_pos = pd.DataFrame({'KeyValue': [f\"{k}:{v}\" for k, v in test_pos_dict.items()]}, index=test_pos_dict.keys())\n",
    "valid_sample_pos = pd.DataFrame({'KeyValue': [f\"{k}:{v}\" for k, v in valid_pos_dict.items()]}, index=valid_pos_dict.keys())\n",
    "\n",
    "print(test_sample_pos[:5])\n",
    "print(valid_sample_pos[:5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:14:49.339965Z",
     "start_time": "2025-08-29T10:14:44.200663Z"
    }
   },
   "id": "a3aa6410c5e91835",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7596\n",
      "4821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_sample_neg = pd.DataFrame.from_dict(test_neg_items).transpose()\n",
    "valid_sample_neg = pd.DataFrame.from_dict(valid_neg_items).transpose()\n",
    "\n",
    "print(len(test_sample_neg))\n",
    "print(len(valid_sample_neg))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:15:15.129707Z",
     "start_time": "2025-08-29T10:14:54.794678Z"
    }
   },
   "id": "e5ad2120a7592c7e",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7596\n",
      "4821\n"
     ]
    }
   ],
   "source": [
    "test_sample_concat = pd.concat([test_sample_pos, test_sample_neg], axis=1)\n",
    "print(len(test_sample_concat))\n",
    "\n",
    "valid_sample_concat = pd.concat([valid_sample_pos, valid_sample_neg], axis=1)\n",
    "print(len(valid_sample_concat))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:15:15.183075Z",
     "start_time": "2025-08-29T10:15:15.125545Z"
    }
   },
   "id": "53c96122e00646db",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            KeyValue      0      1      2  \\\n",
      "0  (0:590,632,1326,1691,2121,2300,2745,2849,3197,...  16385   6146      4   \n",
      "1                                          (1:24125)  22528   4097  12291   \n",
      "2                                      (2:2374,5285)   4101  20485   6159   \n",
      "3                                           (3:3172)   6145  22532   6149   \n",
      "4                              (4:10170,18628,23178)   8192   2050  16386   \n",
      "\n",
      "       3  \n",
      "0  18444  \n",
      "1  24582  \n",
      "2  12305  \n",
      "3  14343  \n",
      "4   4101  \n",
      "                                            KeyValue      0      1      2  \\\n",
      "0  (0:76,3740,6171,6584,6867,6886,8996,15281,1792...   8192  18441   6156   \n",
      "2                                       (2:57,10237)  14336   6144   4101   \n",
      "4                                           (4:3090)  12291  16389   4101   \n",
      "7                               (7:2432,16061,24274)  14336  12288  10245   \n",
      "9      (9:10735,11113,12140,12311,13840,19339,19764)  20480   2053  12298   \n",
      "\n",
      "       3  \n",
      "0  12305  \n",
      "2   8198  \n",
      "4   6149  \n",
      "7  24589  \n",
      "9  18448  \n"
     ]
    }
   ],
   "source": [
    "test_sample_concat['KeyValue'] = test_sample_concat['KeyValue'].apply(lambda x: x.replace('[', '').replace(']', '').replace(' ', ''))\n",
    "test_sample_concat['KeyValue'] = test_sample_concat['KeyValue'].apply(lambda x: f\"({x})\")\n",
    "\n",
    "print(test_sample_concat.iloc[:5, :5])\n",
    "\n",
    "valid_sample_concat['KeyValue'] = valid_sample_concat['KeyValue'].apply(lambda x: x.replace('[', '').replace(']', '').replace(' ', ''))\n",
    "valid_sample_concat['KeyValue'] = valid_sample_concat['KeyValue'].apply(lambda x: f\"({x})\")\n",
    "\n",
    "print(valid_sample_concat.iloc[:5, :5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:15:15.599092Z",
     "start_time": "2025-08-29T10:15:15.182974Z"
    }
   },
   "id": "56c25a6ba77d6c28",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "valid_sample_concat.to_csv(file_path+'data.valid.negative', sep='\t', header=False, index=False)\n",
    "test_sample_concat.to_csv(file_path+'data.test.negative', sep='\t', header=False, index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:15:50.697963Z",
     "start_time": "2025-08-29T10:15:48.441200Z"
    }
   },
   "id": "c7759914d1ecdde5",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "src = train_to_save['u'].values\n",
    "tgt = train_to_save['i'].values\n",
    "data = np.ones(len(train_to_save))\n",
    "mat = coo_matrix((data, (src, tgt)), shape=(len(user_to_idx), len(item_to_idx)))\n",
    "\n",
    "# 指定保存到的文件名\n",
    "import pickle\n",
    "filename = file_path+'inter_coo_matrix.pkl'\n",
    "\n",
    "# 将字典保存到 pkl 文件中\n",
    "with open(filename, 'wb') as f:  # 注意要以二进制写入模式打开文件\n",
    "    pickle.dump(mat, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:18:39.445570Z",
     "start_time": "2025-08-29T10:18:39.383660Z"
    }
   },
   "id": "b565d42851c0243a",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191893, 2)\n",
      "NumPy array saved as 'ur_graph.txt'\n",
      "testing for shape, ur_graph is (191893, 2)\n"
     ]
    }
   ],
   "source": [
    "# now, begin process graph edges\n",
    "# 1 user-recipe graph edge\n",
    "import numpy as np\n",
    "\n",
    "inter_mat = []\n",
    "lines = open(file_path + 'data.train.rating', 'r').readlines()\n",
    "for line in lines:\n",
    "    tokens = line.strip().split('\\t')\n",
    "    u_id, pos_id = int(tokens[0]), int(tokens[1])\n",
    "    inter_mat.append([u_id, pos_id])\n",
    "\n",
    "ur_array = np.array(inter_mat).astype(np.int_)\n",
    "\n",
    "print(ur_array.shape)\n",
    "\n",
    "ur_path = file_path + 'graph_edge/ur_graph.txt'\n",
    "with open(ur_path, 'w') as f:\n",
    "    np.savetxt(f, ur_array, fmt='%d')\n",
    "print(\"NumPy array saved as 'ur_graph.txt'\")\n",
    "\n",
    "with open(ur_path, 'r') as f:\n",
    "    ab = np.loadtxt(f, dtype=np.int_)\n",
    "print(f\"testing for shape, ur_graph is {ab.shape}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:19:45.020446Z",
     "start_time": "2025-08-29T10:19:44.632612Z"
    }
   },
   "id": "659f0e84f99a175",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4277 2698 1845 ... 4963 4963 4963]\n",
      " [1789  510  776 ... 4963 4963 4963]\n",
      " [ 274 4681 1495 ... 4963 4963 4963]\n",
      " ...\n",
      " [4641 4265 4762 ... 4963 4963 4963]\n",
      " [4951 3427 4732 ... 4963 4963 4963]\n",
      " [1024 3678 3941 ... 4963 4963 4963]] 4963 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29943it [01:42, 292.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264041\n",
      "(264041, 2)\n",
      "NumPy array saved as 'ri_graph.txt'\n",
      "testing for shape, ri_graph is (264041, 2)\n"
     ]
    }
   ],
   "source": [
    "# 2 recipe-recipe graph edge \n",
    "# 2 pending\n",
    "# 3 recipe-ingredient graph edge\n",
    "from tqdm import tqdm\n",
    "ingre_code = np.load(file_path + 'data_ingre_code_file.npy')\n",
    "print(ingre_code, np.max(ingre_code), np.min(ingre_code))\n",
    "ri_list = []\n",
    "for i, sublist in tqdm(enumerate(ingre_code)):\n",
    "    for j in sublist:\n",
    "        if j!=np.max(ingre_code):\n",
    "            ri_list.append([i, j])\n",
    "print(len(ri_list))\n",
    "ri_array = np.array(ri_list).astype(np.int_)\n",
    "print(ri_array.shape)\n",
    "\n",
    "ri_path = file_path + 'graph_edge/ri_graph.txt'\n",
    "with open(ri_path, 'w') as f:\n",
    "    np.savetxt(f, ri_array, fmt='%d')\n",
    "print(\"NumPy array saved as 'ri_graph.txt'\")\n",
    "\n",
    "with open(ri_path, 'r') as f:\n",
    "    ab = np.loadtxt(f, dtype=np.int_)\n",
    "print(f\"testing for shape, ri_graph is {ab.shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:21:45.463118Z",
     "start_time": "2025-08-29T10:20:02.532407Z"
    }
   },
   "id": "25fce3985d1b9856",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4963/4963 [00:02<00:00, 2250.28it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4963/4963 [00:00<00:00, 1033273.64it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4963/4963 [00:00<00:00, 840588.38it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4963/4963 [00:00<00:00, 893558.15it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 387.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 2024.12it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 5485.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38235\n",
      "每个元素出现的次数： Counter({1: 37810, 2: 423, 3: 2})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38235/38235 [00:00<00:00, 73990.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76470\n",
      "[[26, 27], [27, 26], [26, 28], [28, 26], [26, 172], [172, 26], [26, 341], [341, 26], [26, 457], [457, 26]]\n",
      "(76470, 2)\n",
      "NumPy array saved as /home/zyx2509/Food_dataset/Foodcom/processed_dataset/graph_edge/ii_graph.txt\n",
      "(76470, 2)\n"
     ]
    }
   ],
   "source": [
    "# 4 ingredient-ingredient graph edge\n",
    "# 4 used for fgcn model\n",
    "from collections import Counter, defaultdict\n",
    "with open('/home/zyx2509/Food_dataset/Foodcom/raw_dataset/ingr_map.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "mapping_dict = file_path + '/mapping_dict.pkl'\n",
    "\n",
    "with open(mapping_dict, 'rb') as f:\n",
    "    u_to_idx, i_to_idx, ingre_to_idx = pickle.load(f)\n",
    "\n",
    "ingreText_to_idx = {}\n",
    "for key, value in tqdm(ingre_to_idx.items()):\n",
    "    result = ''\n",
    "    rows_data = data[data['id']==key]\n",
    "    for index, row in rows_data.iterrows():\n",
    "        result += row['processed']\n",
    "    ingreText_to_idx[result] = value\n",
    "\n",
    "color_set = ['white', 'black', 'red', 'green', 'yellow']\n",
    "ingre_with_colors = defaultdict(list)\n",
    "\n",
    "# 遍历字典中的每个值\n",
    "for key, value in tqdm(ingreText_to_idx.items()):\n",
    "    # 检查值中是否包含颜色列表中的任何一个颜色\n",
    "    for color in color_set:\n",
    "        if color in key:\n",
    "            # 如果包含颜色，则将对应的键保存到列表中\n",
    "            ingre_with_colors[color].append(value)\n",
    "\n",
    "shape_set = ['slice', 'dice', 'minced', 'powder', 'roll', 'shred']\n",
    "ingre_with_shapes = defaultdict(list)\n",
    "\n",
    "# 遍历字典中的每个值\n",
    "for key, value in tqdm(ingreText_to_idx.items()):\n",
    "    # 检查值中是否包含颜色列表中的任何一个颜色\n",
    "    for shape in shape_set:\n",
    "        if shape in key:\n",
    "            # 如果包含颜色，则将对应的键保存到列表中\n",
    "            ingre_with_shapes[shape].append(value)\n",
    "\n",
    "cooking_set = ['deep-fry', 'dry', 'fry', 'steam', 'boil', 'pickle']\n",
    "ingre_with_cookings = defaultdict(list)\n",
    "\n",
    "# 遍历字典中的每个值\n",
    "for key, value in tqdm(ingreText_to_idx.items()):\n",
    "    # 检查值中是否包含颜色列表中的任何一个颜色\n",
    "    for cooking in cooking_set:\n",
    "        if cooking in key:\n",
    "            # 如果包含颜色，则将对应的键保存到列表中\n",
    "            ingre_with_cookings[cooking].append(value)\n",
    "\n",
    "edge_dict = defaultdict(int)\n",
    "\n",
    "for key, lst in tqdm(ingre_with_colors.items()):\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(i + 1, len(lst)):\n",
    "            edge_dict[(lst[i],lst[j])] += 1\n",
    "for key, lst in tqdm(ingre_with_shapes.items()):\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(i + 1, len(lst)):\n",
    "            edge_dict[(lst[i],lst[j])] += 1\n",
    "for key, lst in tqdm(ingre_with_cookings.items()):\n",
    "    for i in range(len(lst)):\n",
    "        for j in range(i + 1, len(lst)):\n",
    "            edge_dict[(lst[i],lst[j])] += 1\n",
    "print(len(edge_dict))\n",
    "value_counts = Counter(val for val in edge_dict.values())\n",
    "\n",
    "# 打印结果\n",
    "print(\"每个元素出现的次数：\", value_counts)\n",
    "\n",
    "new_edge_list = []\n",
    "for edge_pair, count in tqdm(edge_dict.items()):\n",
    "    new_edge_list.append([edge_pair[0], edge_pair[1]])\n",
    "    new_edge_list.append([edge_pair[1], edge_pair[0]])\n",
    "\n",
    "print(len(new_edge_list))\n",
    "print(new_edge_list[:10])\n",
    "\n",
    "ii_array = np.array(new_edge_list).astype(np.int_)\n",
    "\n",
    "print(ii_array.shape)\n",
    "\n",
    "ii_path = file_path + 'graph_edge/ii_graph.txt'\n",
    "with open(ii_path, 'w') as f:\n",
    "    np.savetxt(f, ii_array, fmt='%d')\n",
    "print(f\"NumPy array saved as {ii_path}\")\n",
    "\n",
    "with open(ii_path, 'r') as f:\n",
    "    ab = np.loadtxt(f, dtype=np.int_)\n",
    "print(ab.shape)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:22:25.205288Z",
     "start_time": "2025-08-29T10:22:22.213367Z"
    }
   },
   "id": "ded2aba145e42172",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1740231/3030144398.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  now_recipe['cal'] = now_recipe['nutrition'].apply(lambda x: int(eval(x)[0]/50))\n",
      "/tmp/ipykernel_1740231/3030144398.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  now_recipe['i'] = now_recipe['id'].apply(lambda x: item_to_idx[x])\n",
      "/tmp/ipykernel_1740231/3030144398.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  now_recipe['cal_norm'] = now_recipe['cal'].apply(lambda x: cal_to_idx[x])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "773 0 8.005844437765086\n",
      "983\n",
      "(29943, 2)\n",
      "NumPy array saved as 'rc_graph.txt'\n",
      "testing for shape, rc_graph is (29943, 2)\n",
      "save calories dict\n",
      "save cal_to_idx dict\n"
     ]
    }
   ],
   "source": [
    "# 5 recipe-calories graph edge\n",
    "# 7 calories level dict\n",
    "# 5 and 7 used for schgn model\n",
    "# build calories dict for 5 and 7\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "calories_dict = {}\n",
    "raw_recipe = pd.read_csv('/home/zyx2509/Food_dataset/Foodcom/raw_dataset/RAW_recipes.csv')\n",
    "with open(file_path+'mapping_dict.pkl', 'rb') as f:\n",
    "    _, item_to_idx, _ = pickle.load(f)\n",
    "\n",
    "items = item_to_idx.keys()\n",
    "print(len(items))\n",
    "now_recipe = raw_recipe[raw_recipe['id'].isin(items)]\n",
    "now_recipe['cal'] = now_recipe['nutrition'].apply(lambda x: int(eval(x)[0]/50))\n",
    "now_recipe['i'] = now_recipe['id'].apply(lambda x: item_to_idx[x])\n",
    "print(now_recipe['cal'].max(), now_recipe['cal'].min(), now_recipe['cal'].mean())\n",
    "print((now_recipe['cal'] == 0).sum())\n",
    "\n",
    "cal_encoder = LabelEncoder().fit(list(now_recipe['cal'].unique()))\n",
    "cal_to_idx = {v: i for i, v in enumerate(cal_encoder.classes_)}\n",
    "\n",
    "now_recipe['cal_norm'] = now_recipe['cal'].apply(lambda x: cal_to_idx[x])\n",
    "now_recipe = now_recipe.sort_values(by='i')\n",
    "\n",
    "for index, row in now_recipe.iterrows():\n",
    "    recipe_id = row['i']\n",
    "    calories_amount = row['cal_norm']\n",
    "    calories_dict[recipe_id] = calories_amount\n",
    "# 5 recipe-calories graph edge\n",
    "rc_list = []\n",
    "for key, value in calories_dict.items():\n",
    "    rc_list.append([key, value])\n",
    "rc_array = np.array(rc_list).astype(np.int_)\n",
    "print(rc_array.shape)\n",
    "\n",
    "rc_path = file_path + 'graph_edge/rc_graph.txt'\n",
    "with open(rc_path, 'w') as f:\n",
    "    np.savetxt(f, rc_array, fmt='%d')\n",
    "print(\"NumPy array saved as 'rc_graph.txt'\")\n",
    "\n",
    "with open(rc_path, 'r') as f:\n",
    "    ab = np.loadtxt(f, dtype=np.int_)\n",
    "print(f\"testing for shape, rc_graph is {ab.shape}\")\n",
    "\n",
    "# 7 calories level dict\n",
    "rc_dict = file_path + 'graph_edge/recipe_cal_level_dict.pkl'\n",
    "\n",
    "# 将字典保存到 pkl 文件中\n",
    "with open(rc_dict, 'wb') as f:  # 注意要以二进制写入模式打开文件\n",
    "    pickle.dump(calories_dict, f)\n",
    "print(\"save calories dict\")\n",
    "\n",
    "cal_to_idx_map_file = file_path + 'graph_edge/recipe_cal_level_map.pkl'\n",
    "with open(cal_to_idx_map_file, 'wb') as f:  # 注意要以二进制写入模式打开文件\n",
    "    pickle.dump(cal_to_idx, f)\n",
    "print(\"save cal_to_idx dict\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:22:36.358588Z",
     "start_time": "2025-08-29T10:22:29.672121Z"
    }
   },
   "id": "ceec286f464528c8",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1740231/101115609.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['i'] = recipe_nutrition['id'].apply(lambda x: item_to_idx[x])\n",
      "/tmp/ipykernel_1740231/101115609.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['cal'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[0])\n",
      "/tmp/ipykernel_1740231/101115609.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['fat'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[1])\n",
      "/tmp/ipykernel_1740231/101115609.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['sugar'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[2])\n",
      "/tmp/ipykernel_1740231/101115609.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['sodium'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[3])\n",
      "/tmp/ipykernel_1740231/101115609.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['protein'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[4])\n",
      "/tmp/ipykernel_1740231/101115609.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['saturated_fat'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[5])\n",
      "/tmp/ipykernel_1740231/101115609.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition['carbohydrates'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[6])\n",
      "0it [00:00, ?it/s]/tmp/ipykernel_1740231/101115609.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  recipe_nutrition.at[idx, 'health'] = health_level\n",
      "29943it [00:02, 11792.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "            id                                     nutrition      i    cal  \\\n15       63986        [105.7, 8.0, 0.0, 26.0, 5.0, 4.0, 3.0]   6678  105.7   \n16       43026      [94.0, 10.0, 0.0, 11.0, 11.0, 21.0, 0.0]   4371   94.0   \n33       54100    [190.9, 10.0, 10.0, 10.0, 45.0, 15.0, 2.0]   5526  190.9   \n36       25775     [166.1, 16.0, 6.0, 32.0, 19.0, 26.0, 3.0]   2420  166.1   \n40       90921  [783.4, 46.0, 107.0, 56.0, 36.0, 21.0, 37.0]   9317  783.4   \n...        ...                                           ...    ...    ...   \n231564  283432     [162.2, 16.0, 17.0, 9.0, 10.0, 30.0, 4.0]  23716  162.2   \n231566   96811      [145.7, 13.0, 10.0, 2.0, 3.0, 27.0, 5.0]   9987  145.7   \n231572  455000         [94.0, 7.0, 25.0, 4.0, 4.0, 3.0, 4.0]  29240   94.0   \n231600  208429   [432.8, 32.0, 29.0, 39.0, 42.0, 39.0, 15.0]  19330  432.8   \n231634  308080          [59.2, 6.0, 2.0, 3.0, 6.0, 5.0, 0.0]  24929   59.2   \n\n         fat  sugar  sodium  protein  saturated_fat  carbohydrates  health  \n15       8.0    0.0    26.0      5.0            4.0            3.0     3.0  \n16      10.0    0.0    11.0     11.0           21.0            0.0     3.0  \n33      10.0   10.0    10.0     45.0           15.0            2.0     1.0  \n36      16.0    6.0    32.0     19.0           26.0            3.0     3.0  \n40      46.0  107.0    56.0     36.0           21.0           37.0     1.0  \n...      ...    ...     ...      ...            ...            ...     ...  \n231564  16.0   17.0     9.0     10.0           30.0            4.0     3.0  \n231566  13.0   10.0     2.0      3.0           27.0            5.0     1.0  \n231572   7.0   25.0     4.0      4.0            3.0            4.0     2.0  \n231600  32.0   29.0    39.0     42.0           39.0           15.0     1.0  \n231634   6.0    2.0     3.0      6.0            5.0            0.0     3.0  \n\n[29943 rows x 11 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>nutrition</th>\n      <th>i</th>\n      <th>cal</th>\n      <th>fat</th>\n      <th>sugar</th>\n      <th>sodium</th>\n      <th>protein</th>\n      <th>saturated_fat</th>\n      <th>carbohydrates</th>\n      <th>health</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>15</th>\n      <td>63986</td>\n      <td>[105.7, 8.0, 0.0, 26.0, 5.0, 4.0, 3.0]</td>\n      <td>6678</td>\n      <td>105.7</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>26.0</td>\n      <td>5.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>43026</td>\n      <td>[94.0, 10.0, 0.0, 11.0, 11.0, 21.0, 0.0]</td>\n      <td>4371</td>\n      <td>94.0</td>\n      <td>10.0</td>\n      <td>0.0</td>\n      <td>11.0</td>\n      <td>11.0</td>\n      <td>21.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>54100</td>\n      <td>[190.9, 10.0, 10.0, 10.0, 45.0, 15.0, 2.0]</td>\n      <td>5526</td>\n      <td>190.9</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>10.0</td>\n      <td>45.0</td>\n      <td>15.0</td>\n      <td>2.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>25775</td>\n      <td>[166.1, 16.0, 6.0, 32.0, 19.0, 26.0, 3.0]</td>\n      <td>2420</td>\n      <td>166.1</td>\n      <td>16.0</td>\n      <td>6.0</td>\n      <td>32.0</td>\n      <td>19.0</td>\n      <td>26.0</td>\n      <td>3.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>90921</td>\n      <td>[783.4, 46.0, 107.0, 56.0, 36.0, 21.0, 37.0]</td>\n      <td>9317</td>\n      <td>783.4</td>\n      <td>46.0</td>\n      <td>107.0</td>\n      <td>56.0</td>\n      <td>36.0</td>\n      <td>21.0</td>\n      <td>37.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>231564</th>\n      <td>283432</td>\n      <td>[162.2, 16.0, 17.0, 9.0, 10.0, 30.0, 4.0]</td>\n      <td>23716</td>\n      <td>162.2</td>\n      <td>16.0</td>\n      <td>17.0</td>\n      <td>9.0</td>\n      <td>10.0</td>\n      <td>30.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>231566</th>\n      <td>96811</td>\n      <td>[145.7, 13.0, 10.0, 2.0, 3.0, 27.0, 5.0]</td>\n      <td>9987</td>\n      <td>145.7</td>\n      <td>13.0</td>\n      <td>10.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>27.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>231572</th>\n      <td>455000</td>\n      <td>[94.0, 7.0, 25.0, 4.0, 4.0, 3.0, 4.0]</td>\n      <td>29240</td>\n      <td>94.0</td>\n      <td>7.0</td>\n      <td>25.0</td>\n      <td>4.0</td>\n      <td>4.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>231600</th>\n      <td>208429</td>\n      <td>[432.8, 32.0, 29.0, 39.0, 42.0, 39.0, 15.0]</td>\n      <td>19330</td>\n      <td>432.8</td>\n      <td>32.0</td>\n      <td>29.0</td>\n      <td>39.0</td>\n      <td>42.0</td>\n      <td>39.0</td>\n      <td>15.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>231634</th>\n      <td>308080</td>\n      <td>[59.2, 6.0, 2.0, 3.0, 6.0, 5.0, 0.0]</td>\n      <td>24929</td>\n      <td>59.2</td>\n      <td>6.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>6.0</td>\n      <td>5.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>29943 rows × 11 columns</p>\n</div>"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# begin to calculate health level\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# 将字典保存到 pkl 文件中\n",
    "raw_recipe = pd.read_csv('/home/zyx2509/Food_dataset/Foodcom/raw_dataset/RAW_recipes.csv')\n",
    "with open(file_path+'mapping_dict.pkl', 'rb') as f:\n",
    "    _, item_to_idx, _ = pickle.load(f)\n",
    "dataset_recipe = raw_recipe[raw_recipe['id'].isin(list(item_to_idx.keys()))]\n",
    "recipe_nutrition = dataset_recipe[['id', 'nutrition']]\n",
    "recipe_nutrition['i'] = recipe_nutrition['id'].apply(lambda x: item_to_idx[x])\n",
    "recipe_nutrition['cal'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[0])\n",
    "recipe_nutrition['fat'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[1])\n",
    "recipe_nutrition['sugar'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[2])\n",
    "recipe_nutrition['sodium'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[3])\n",
    "recipe_nutrition['protein'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[4])\n",
    "recipe_nutrition['saturated_fat'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[5])\n",
    "recipe_nutrition['carbohydrates'] = recipe_nutrition['nutrition'].apply(lambda x: eval(x)[6])\n",
    "\n",
    "for idx, row in tqdm(recipe_nutrition.iterrows()):\n",
    "    health_level = 0\n",
    "    # 65g\n",
    "    if 15<=row['fat']<=30:\n",
    "        health_level+=1\n",
    "    # 50g\n",
    "    if row['sugar']<10:\n",
    "        health_level+=1\n",
    "    # WHO healthy sodium 2g per day, daily 2400mg, 2/2.4=83\n",
    "    # WHO healthy salt 5g per day, daily 6000mg\n",
    "    if row['sodium']<83:\n",
    "        health_level+=1\n",
    "    # 50g\n",
    "    if 10<=row['protein']<=15:\n",
    "        health_level+=1\n",
    "    # 20g\n",
    "    if row['saturated_fat']<10:\n",
    "        health_level+=1 \n",
    "    # 300g\n",
    "    if 55<=row['carbohydrates']<=75:\n",
    "        health_level+=1\n",
    "    recipe_nutrition.at[idx, 'health'] = health_level\n",
    "recipe_nutrition"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:23:43.181494Z",
     "start_time": "2025-08-29T10:23:32.784996Z"
    }
   },
   "id": "742c83a6f6bede37",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0]    7658\n",
      "[0, 0, 1, 0, 1, 0]    4868\n",
      "[1, 0, 1, 0, 0, 0]    4365\n",
      "[0, 1, 1, 0, 0, 0]    2957\n",
      "[0, 1, 1, 0, 1, 0]    2216\n",
      "[1, 1, 1, 0, 0, 0]    1681\n",
      "[0, 0, 1, 1, 0, 0]    1143\n",
      "[0, 0, 0, 0, 0, 0]    1036\n",
      "[1, 0, 1, 1, 0, 0]     858\n",
      "[0, 0, 1, 1, 1, 0]     661\n",
      "[1, 0, 1, 0, 1, 0]     501\n",
      "[0, 1, 1, 1, 1, 0]     350\n",
      "[0, 1, 1, 1, 0, 0]     301\n",
      "[1, 1, 1, 1, 0, 0]     286\n",
      "[1, 1, 1, 0, 1, 0]     197\n",
      "[0, 0, 0, 0, 1, 0]     128\n",
      "[0, 0, 1, 0, 0, 1]     125\n",
      "[1, 0, 0, 0, 0, 0]     105\n",
      "[1, 0, 1, 1, 1, 0]     103\n",
      "[0, 1, 0, 0, 0, 0]      97\n",
      "[0, 0, 1, 0, 1, 1]      54\n",
      "[0, 0, 0, 0, 0, 1]      44\n",
      "[0, 0, 0, 1, 1, 0]      35\n",
      "[0, 1, 0, 0, 1, 0]      30\n",
      "[1, 1, 1, 1, 1, 0]      28\n",
      "[1, 0, 0, 0, 1, 0]      26\n",
      "[1, 1, 0, 0, 0, 0]      21\n",
      "[1, 0, 1, 0, 0, 1]      17\n",
      "[0, 0, 0, 1, 0, 0]      10\n",
      "[1, 0, 0, 1, 1, 0]       4\n",
      "[0, 1, 0, 1, 1, 0]       3\n",
      "[0, 0, 1, 1, 1, 1]       3\n",
      "[1, 1, 0, 0, 1, 0]       3\n",
      "[1, 1, 1, 0, 0, 1]       3\n",
      "[1, 0, 0, 0, 0, 1]       3\n",
      "[0, 1, 0, 0, 0, 1]       3\n",
      "[0, 1, 1, 0, 0, 1]       3\n",
      "[0, 0, 1, 1, 0, 1]       3\n",
      "[0, 0, 0, 0, 1, 1]       3\n",
      "[0, 1, 1, 0, 1, 1]       2\n",
      "[0, 0, 0, 1, 1, 1]       2\n",
      "[1, 0, 0, 1, 0, 0]       2\n",
      "[1, 1, 0, 0, 0, 1]       1\n",
      "[1, 1, 0, 1, 0, 0]       1\n",
      "[0, 1, 0, 1, 0, 0]       1\n",
      "[0, 0, 0, 1, 0, 1]       1\n",
      "[1, 0, 1, 1, 1, 1]       1\n",
      "Name: nut, dtype: int64\n",
      "save health dict\n"
     ]
    },
    {
     "data": {
      "text/plain": "           id                                    nutrition      i    cal  \\\n0          40     [311.1, 0.0, 308.0, 0.0, 0.0, 0.0, 27.0]      0  311.1   \n1          49   [627.7, 38.0, 8.0, 35.0, 115.0, 64.0, 4.0]      1  627.7   \n2          58     [280.1, 9.0, 36.0, 24.0, 59.0, 7.0, 7.0]      2  280.1   \n3          66   [772.0, 6.0, 657.0, 93.0, 13.0, 2.0, 63.0]      3  772.0   \n4         142    [224.8, 14.0, 87.0, 10.0, 7.0, 9.0, 11.0]      4  224.8   \n...       ...                                          ...    ...    ...   \n29938  522871       [63.7, 0.0, 20.0, 43.0, 8.0, 0.0, 3.0]  29938   63.7   \n29939  522889   [551.0, 71.0, 2.0, 42.0, 60.0, 127.0, 0.0]  29939  551.0   \n29940  532736     [139.0, 11.0, 56.0, 2.0, 5.0, 22.0, 5.0]  29940  139.0   \n29941  532740  [415.6, 27.0, 222.0, 4.0, 22.0, 30.0, 19.0]  29941  415.6   \n29942  536119    [247.9, 30.0, 28.0, 21.0, 5.0, 14.0, 5.0]  29942  247.9   \n\n        fat  sugar  sodium  protein  saturated_fat  carbohydrates  health  n1  \\\n0       0.0  308.0     0.0      0.0            0.0           27.0     2.0   0   \n1      38.0    8.0    35.0    115.0           64.0            4.0     2.0   0   \n2       9.0   36.0    24.0     59.0            7.0            7.0     2.0   0   \n3       6.0  657.0    93.0     13.0            2.0           63.0     3.0   0   \n4      14.0   87.0    10.0      7.0            9.0           11.0     2.0   0   \n...     ...    ...     ...      ...            ...            ...     ...  ..   \n29938   0.0   20.0    43.0      8.0            0.0            3.0     2.0   0   \n29939  71.0    2.0    42.0     60.0          127.0            0.0     2.0   0   \n29940  11.0   56.0     2.0      5.0           22.0            5.0     1.0   0   \n29941  27.0  222.0     4.0     22.0           30.0           19.0     2.0   1   \n29942  30.0   28.0    21.0      5.0           14.0            5.0     2.0   1   \n\n       n2  n3  n4  n5  n6                 nut  \n0       0   1   0   1   0  [0, 0, 1, 0, 1, 0]  \n1       1   1   0   0   0  [0, 1, 1, 0, 0, 0]  \n2       0   1   0   1   0  [0, 0, 1, 0, 1, 0]  \n3       0   0   1   1   1  [0, 0, 0, 1, 1, 1]  \n4       0   1   0   1   0  [0, 0, 1, 0, 1, 0]  \n...    ..  ..  ..  ..  ..                 ...  \n29938   0   1   0   1   0  [0, 0, 1, 0, 1, 0]  \n29939   1   1   0   0   0  [0, 1, 1, 0, 0, 0]  \n29940   0   1   0   0   0  [0, 0, 1, 0, 0, 0]  \n29941   0   1   0   0   0  [1, 0, 1, 0, 0, 0]  \n29942   0   1   0   0   0  [1, 0, 1, 0, 0, 0]  \n\n[29943 rows x 18 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>nutrition</th>\n      <th>i</th>\n      <th>cal</th>\n      <th>fat</th>\n      <th>sugar</th>\n      <th>sodium</th>\n      <th>protein</th>\n      <th>saturated_fat</th>\n      <th>carbohydrates</th>\n      <th>health</th>\n      <th>n1</th>\n      <th>n2</th>\n      <th>n3</th>\n      <th>n4</th>\n      <th>n5</th>\n      <th>n6</th>\n      <th>nut</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>40</td>\n      <td>[311.1, 0.0, 308.0, 0.0, 0.0, 0.0, 27.0]</td>\n      <td>0</td>\n      <td>311.1</td>\n      <td>0.0</td>\n      <td>308.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>27.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 1, 0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>49</td>\n      <td>[627.7, 38.0, 8.0, 35.0, 115.0, 64.0, 4.0]</td>\n      <td>1</td>\n      <td>627.7</td>\n      <td>38.0</td>\n      <td>8.0</td>\n      <td>35.0</td>\n      <td>115.0</td>\n      <td>64.0</td>\n      <td>4.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 1, 1, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>58</td>\n      <td>[280.1, 9.0, 36.0, 24.0, 59.0, 7.0, 7.0]</td>\n      <td>2</td>\n      <td>280.1</td>\n      <td>9.0</td>\n      <td>36.0</td>\n      <td>24.0</td>\n      <td>59.0</td>\n      <td>7.0</td>\n      <td>7.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 1, 0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>66</td>\n      <td>[772.0, 6.0, 657.0, 93.0, 13.0, 2.0, 63.0]</td>\n      <td>3</td>\n      <td>772.0</td>\n      <td>6.0</td>\n      <td>657.0</td>\n      <td>93.0</td>\n      <td>13.0</td>\n      <td>2.0</td>\n      <td>63.0</td>\n      <td>3.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>[0, 0, 0, 1, 1, 1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>142</td>\n      <td>[224.8, 14.0, 87.0, 10.0, 7.0, 9.0, 11.0]</td>\n      <td>4</td>\n      <td>224.8</td>\n      <td>14.0</td>\n      <td>87.0</td>\n      <td>10.0</td>\n      <td>7.0</td>\n      <td>9.0</td>\n      <td>11.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 1, 0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29938</th>\n      <td>522871</td>\n      <td>[63.7, 0.0, 20.0, 43.0, 8.0, 0.0, 3.0]</td>\n      <td>29938</td>\n      <td>63.7</td>\n      <td>0.0</td>\n      <td>20.0</td>\n      <td>43.0</td>\n      <td>8.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>[0, 0, 1, 0, 1, 0]</td>\n    </tr>\n    <tr>\n      <th>29939</th>\n      <td>522889</td>\n      <td>[551.0, 71.0, 2.0, 42.0, 60.0, 127.0, 0.0]</td>\n      <td>29939</td>\n      <td>551.0</td>\n      <td>71.0</td>\n      <td>2.0</td>\n      <td>42.0</td>\n      <td>60.0</td>\n      <td>127.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 1, 1, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>29940</th>\n      <td>532736</td>\n      <td>[139.0, 11.0, 56.0, 2.0, 5.0, 22.0, 5.0]</td>\n      <td>29940</td>\n      <td>139.0</td>\n      <td>11.0</td>\n      <td>56.0</td>\n      <td>2.0</td>\n      <td>5.0</td>\n      <td>22.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[0, 0, 1, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>29941</th>\n      <td>532740</td>\n      <td>[415.6, 27.0, 222.0, 4.0, 22.0, 30.0, 19.0]</td>\n      <td>29941</td>\n      <td>415.6</td>\n      <td>27.0</td>\n      <td>222.0</td>\n      <td>4.0</td>\n      <td>22.0</td>\n      <td>30.0</td>\n      <td>19.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[1, 0, 1, 0, 0, 0]</td>\n    </tr>\n    <tr>\n      <th>29942</th>\n      <td>536119</td>\n      <td>[247.9, 30.0, 28.0, 21.0, 5.0, 14.0, 5.0]</td>\n      <td>29942</td>\n      <td>247.9</td>\n      <td>30.0</td>\n      <td>28.0</td>\n      <td>21.0</td>\n      <td>5.0</td>\n      <td>14.0</td>\n      <td>5.0</td>\n      <td>2.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>[1, 0, 1, 0, 0, 0]</td>\n    </tr>\n  </tbody>\n</table>\n<p>29943 rows × 18 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recipe_nutrition['n1'] = recipe_nutrition['fat'].apply(lambda x:1 if 15<=x<=30 else 0)\n",
    "recipe_nutrition['n2'] = recipe_nutrition['sugar'].apply(lambda x:1 if x<10 else 0)\n",
    "recipe_nutrition['n3'] = recipe_nutrition['sodium'].apply(lambda x:1 if x<83 else 0)\n",
    "recipe_nutrition['n4'] = recipe_nutrition['protein'].apply(lambda x:1 if 10<=x<=15 else 0)\n",
    "recipe_nutrition['n5'] = recipe_nutrition['saturated_fat'].apply(lambda x:1 if x<10 else 0)\n",
    "recipe_nutrition['n6'] = recipe_nutrition['carbohydrates'].apply(lambda x:1 if 55<=x<=75 else 0)\n",
    "recipe_nutrition = recipe_nutrition.sort_values(by='i').reset_index(drop=True)\n",
    "numeric_columns = recipe_nutrition.iloc[:, -6:].values.tolist()\n",
    "list_df = pd.DataFrame({'nut':numeric_columns})\n",
    "concat = pd.concat([recipe_nutrition, list_df], axis = 1)\n",
    "print(concat['nut'].value_counts())\n",
    "\n",
    "multi_hot_health_dict = {}\n",
    "for index, row in concat.iterrows():\n",
    "    recipe_id = row['i']\n",
    "    health_list = row['nut']\n",
    "    multi_hot_health_dict[recipe_id] = health_list\n",
    "rh_dict_mh = file_path + 'graph_edge/recipe_health_level_multi_hot_dict.pkl'\n",
    "\n",
    "# 将字典保存到 pkl 文件中\n",
    "with open(rh_dict_mh, 'wb') as f:  # 注意要以二进制写入模式打开文件\n",
    "    pickle.dump(multi_hot_health_dict, f)\n",
    "print(\"save health dict\")\n",
    "concat"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:23:51.181275Z",
     "start_time": "2025-08-29T10:23:49.784055Z"
    }
   },
   "id": "a92482b853f9eb6c",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29943it [00:02, 14037.35it/s]\n",
      "/home/miniconda3/envs/zyx2509/lib/python3.10/site-packages/scipy/sparse/_index.py:146: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 29943/29943 [00:01<00:00, 28480.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge number of graph 306232\n",
      "(306232, 3)\n",
      "NumPy array saved as 'rr_health_graph.txt'\n",
      "testing for shape, rr_health_graph is (306232, 3)\n"
     ]
    }
   ],
   "source": [
    "recipe_nutrition['n1'] = recipe_nutrition['fat'].apply(lambda x:0 if 15<=x<=30 else -1)\n",
    "recipe_nutrition['n2'] = recipe_nutrition['sugar'].apply(lambda x:1 if x<10 else -1)\n",
    "recipe_nutrition['n3'] = recipe_nutrition['sodium'].apply(lambda x:2 if x<83 else -1)\n",
    "recipe_nutrition['n4'] = recipe_nutrition['protein'].apply(lambda x:3 if 10<=x<=15 else -1)\n",
    "recipe_nutrition['n5'] = recipe_nutrition['saturated_fat'].apply(lambda x:4 if x<10 else -1)\n",
    "recipe_nutrition['n6'] = recipe_nutrition['carbohydrates'].apply(lambda x:5 if 55<=x<=75 else -1)\n",
    "recipe_nutrition = recipe_nutrition.sort_values(by='i').reset_index(drop=True)\n",
    "numeric_columns = recipe_nutrition.iloc[:, -6:].values.tolist()\n",
    "list_df = pd.DataFrame({'nut':numeric_columns})\n",
    "# concat = pd.concat([recipe_nutrition, list_df], axis = 1)\n",
    "# print(concat['nut'].value_counts())\n",
    "# concat\n",
    "\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "\n",
    "item_nut_mat = sp.dok_matrix(\n",
    "        (len(item_to_idx), 6), dtype=np.float32\n",
    "    )\n",
    "\n",
    "for item, ingres in tqdm(enumerate(numeric_columns)):\n",
    "    ingre = [i for i in ingres if i!=-1]\n",
    "    item_nut_mat[item, ingre] = 1\n",
    "\n",
    "item_nut_mat_csr = item_nut_mat.tocsr()\n",
    "item_item_mat_ = item_nut_mat_csr @ item_nut_mat_csr.T\n",
    "\n",
    "item_item_mat_.setdiag(0)\n",
    "item_item_mat_.data[item_item_mat_.data<=3]=0\n",
    "item_item_mat_.eliminate_zeros()\n",
    "\n",
    "print(item_item_mat_.nnz)\n",
    "\n",
    "graph_data = []\n",
    "for i, row in enumerate(tqdm(item_item_mat_, total=item_item_mat_.shape[0])):\n",
    "    for j, r in zip(row.indices, row.data):\n",
    "        graph_data.append((i, j, r))\n",
    "print(\"edge number of graph\", len(graph_data))\n",
    "\n",
    "rr_health_array = np.array(graph_data).astype(np.int_)\n",
    "print(rr_health_array.shape)\n",
    "\n",
    "rr_health_path = file_path + 'graph_edge/rr_health_graph.txt'\n",
    "with open(rr_health_path, 'w') as f:\n",
    "    np.savetxt(f, rr_health_array, fmt='%d')\n",
    "print(\"NumPy array saved as 'rr_health_graph.txt'\")\n",
    "\n",
    "with open(rr_health_path, 'r') as f:\n",
    "    ab = np.loadtxt(f, dtype=np.int_)\n",
    "print(f\"testing for shape, rr_health_graph is {ab.shape}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:24:44.670970Z",
     "start_time": "2025-08-29T10:24:24.099730Z"
    }
   },
   "id": "25a712144fb18c5a",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save health dict\n",
      "(29943, 2)\n",
      "NumPy array saved as 'rh_graph.txt'\n",
      "testing for shape, rh_graph is (29943, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": "id                                                           93481\nnutrition        [38662.3, 4331.0, 77.0, 809.0, 6552.0, 4969.0,...\ni                                                             9596\ncal                                                        38662.3\nfat                                                         4331.0\nsugar                                                         77.0\nsodium                                                       809.0\nprotein                                                     6552.0\nsaturated_fat                                               4969.0\ncarbohydrates                                                  8.0\nhealth                                                         0.0\nn1                                                              -1\nn2                                                              -1\nn3                                                              -1\nn4                                                              -1\nn5                                                              -1\nn6                                                              -1\nName: 9596, dtype: object"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "health_dict = {}\n",
    "for index, row in recipe_nutrition.iterrows():\n",
    "    recipe_id = row['i']\n",
    "    health_amount = row['health']\n",
    "    health_dict[recipe_id] = int(health_amount)\n",
    "rh_dict = file_path + 'graph_edge/recipe_health_level_dict.pkl'\n",
    "\n",
    "# 将字典保存到 pkl 文件中\n",
    "with open(rh_dict, 'wb') as f:  # 注意要以二进制写入模式打开文件\n",
    "    pickle.dump(health_dict, f)\n",
    "print(\"save health dict\")\n",
    "\n",
    "# recipe-health graph edge\n",
    "rh_list = []\n",
    "for key, value in health_dict.items():\n",
    "    rh_list.append([key, value])\n",
    "rh_array = np.array(rh_list).astype(np.int_)\n",
    "print(rh_array.shape)\n",
    "\n",
    "rh_path = file_path + 'graph_edge/rh_graph.txt'\n",
    "with open(rh_path, 'w') as f:\n",
    "    np.savetxt(f, rh_array, fmt='%d')\n",
    "print(\"NumPy array saved as 'rh_graph.txt'\")\n",
    "\n",
    "with open(rh_path, 'r') as f:\n",
    "    ab = np.loadtxt(f, dtype=np.int_)\n",
    "print(f\"testing for shape, rh_graph is {ab.shape}\")\n",
    "\n",
    "\n",
    "max_index = recipe_nutrition['protein'].idxmax()  # 找到最大值所在的索引\n",
    "max_row = recipe_nutrition.loc[max_index]  # 获取最大值所在的整行数据\n",
    "max_row"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-08-29T10:25:32.332067Z",
     "start_time": "2025-08-29T10:25:31.386980Z"
    }
   },
   "id": "64a09ff78dbe3029",
   "execution_count": 29
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "zyx2509",
   "language": "python",
   "display_name": "zyx (2509)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
